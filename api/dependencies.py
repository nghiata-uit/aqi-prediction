"""
Dependencies cho API - qu·∫£n l√Ω vi·ªác load models v√† artifacts
Module n√†y load c√°c artifacts khi startup v√† cung c·∫•p c√°c h√†m ƒë·ªÉ access
"""
import sys
from pathlib import Path
from typing import Optional, List
import joblib
from sklearn.preprocessing import StandardScaler
import pandas as pd
import logging

# Th√™m th∆∞ m·ª•c g·ªëc v√†o Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Global variables ƒë·ªÉ cache c√°c artifacts
_model = None
_spatial_scaler = None
_feature_columns = None
_historical_data = None


def load_artifacts(models_dir: Path = None):
    """
    Load t·∫•t c·∫£ artifacts t·ª´ th∆∞ m·ª•c models/ khi startup
    
    Args:
        models_dir: ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a models (m·∫∑c ƒë·ªãnh: project_root/models)
    """
    global _model, _spatial_scaler, _feature_columns
    
    if models_dir is None:
        models_dir = project_root / "models"
    
    try:
        logger.info("üöÄ Loading model artifacts...")
        
        # Load XGBoost global model
        model_path = models_dir / "xgboost_global.pkl"
        if model_path.exists():
            _model = joblib.load(model_path)
            logger.info(f"‚úÖ Loaded model from {model_path}")
        else:
            logger.error(f"‚ùå Model not found at {model_path}")
            
        # Load spatial scaler
        scaler_path = models_dir / "spatial_scaler.pkl"
        if scaler_path.exists():
            _spatial_scaler = joblib.load(scaler_path)
            logger.info(f"‚úÖ Loaded spatial scaler from {scaler_path}")
        else:
            logger.error(f"‚ùå Spatial scaler not found at {scaler_path}")
            
        # Load feature columns
        feature_cols_path = models_dir / "feature_columns_global.pkl"
        if feature_cols_path.exists():
            _feature_columns = joblib.load(feature_cols_path)
            logger.info(f"‚úÖ Loaded feature columns from {feature_cols_path}")
            logger.info(f"   Total features: {len(_feature_columns)}")
        else:
            logger.error(f"‚ùå Feature columns not found at {feature_cols_path}")
            
        logger.info("‚úÖ All artifacts loaded successfully")
        
    except Exception as e:
        logger.error(f"‚ùå Error loading artifacts: {str(e)}")
        raise


def load_historical_data(data_path: Path = None):
    """
    Load historical data ƒë·ªÉ s·ª≠ d·ª•ng cho predictions
    
    Args:
        data_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file d·ªØ li·ªáu (m·∫∑c ƒë·ªãnh: project_root/data/sample_data.csv)
    """
    global _historical_data
    
    if data_path is None:
        data_path = project_root / "data" / "sample_data.csv"
    
    try:
        logger.info(f"üìÇ Loading historical data from {data_path}")
        from src.data_preprocessing import preprocess_data
        _historical_data = preprocess_data(str(data_path))
        logger.info(f"‚úÖ Loaded historical data: {len(_historical_data)} samples")
    except Exception as e:
        logger.error(f"‚ùå Error loading historical data: {str(e)}")
        raise


def get_model():
    """
    L·∫•y model ƒë√£ load
    
    Returns:
        XGBoost model ho·∫∑c None n·∫øu ch∆∞a load
    """
    return _model


def get_scaler() -> Optional[StandardScaler]:
    """
    L·∫•y spatial scaler ƒë√£ load
    
    Returns:
        StandardScaler ho·∫∑c None n·∫øu ch∆∞a load
    """
    return _spatial_scaler


def get_feature_cols() -> Optional[List[str]]:
    """
    L·∫•y danh s√°ch feature columns ƒë√£ load
    
    Returns:
        List feature names ho·∫∑c None n·∫øu ch∆∞a load
    """
    return _feature_columns


def get_historical_data() -> Optional[pd.DataFrame]:
    """
    L·∫•y historical data ƒë√£ load
    
    Returns:
        DataFrame ho·∫∑c None n·∫øu ch∆∞a load
    """
    return _historical_data


def is_ready() -> bool:
    """
    Ki·ªÉm tra xem t·∫•t c·∫£ artifacts ƒë√£ ƒë∆∞·ª£c load ch∆∞a
    
    Returns:
        True n·∫øu t·∫•t c·∫£ artifacts ƒë√£ load, False n·∫øu kh√¥ng
    """
    ready = (_model is not None and 
             _spatial_scaler is not None and 
             _feature_columns is not None)
    
    if not ready:
        logger.warning("‚ö†Ô∏è  Not all artifacts are loaded:")
        logger.warning(f"   Model loaded: {_model is not None}")
        logger.warning(f"   Spatial scaler loaded: {_spatial_scaler is not None}")
        logger.warning(f"   Feature columns loaded: {_feature_columns is not None}")
    
    return ready


# H√†m kh·ªüi t·∫°o ƒë·ªÉ g·ªçi khi startup
def initialize(models_dir: Path = None, data_path: Path = None):
    """
    Kh·ªüi t·∫°o v√† load t·∫•t c·∫£ artifacts khi startup
    
    Args:
        models_dir: ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a models
        data_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file historical data
    """
    logger.info("="*80)
    logger.info("üåç INITIALIZING API DEPENDENCIES")
    logger.info("="*80)
    
    load_artifacts(models_dir)
    load_historical_data(data_path)
    
    if is_ready():
        logger.info("="*80)
        logger.info("‚úÖ API READY TO SERVE REQUESTS")
        logger.info("="*80)
    else:
        logger.error("="*80)
        logger.error("‚ùå API NOT READY - Missing artifacts")
        logger.error("="*80)
        raise RuntimeError("Failed to initialize API dependencies")


if __name__ == "__main__":
    # Test loading
    initialize()
    print(f"\nModel type: {type(get_model())}")
    print(f"Scaler type: {type(get_scaler())}")
    print(f"Feature columns count: {len(get_feature_cols())}")
    print(f"Historical data shape: {get_historical_data().shape}")
    print(f"API ready: {is_ready()}")
